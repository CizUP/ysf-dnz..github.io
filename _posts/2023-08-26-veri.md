--- 
 title: Veri Biliminde Python ile SEO Otomasyonu 
 date: 2023-08-26 14:57:49 
 categories: ['seo', 'python', 'google-ads', 'data-science', 'pandas'] 
 tags: ['seo', 'python', 'google-ads', 'data-science', 'pandas'] 
--- 
>  ___Python Kullanarak Veri Bilimi ile SEO Zorluklarının Çözümü___

### Derin SEO (Veri)&nbsp;Bilimi

Veri bilimini SEO’ya uygulamaya teşvik eden birçok trend bulunmaktadır; ancak önce şuna değinelim: Neden veri bilimcileri SEO endüstrisinin kapısına koşmuyor? Neden bunun yerine ücretli arama, programlı reklamcılık ve kitle planlaması gibi alanlara yöneliyorlar?

*   __Gürültülü geri besleme&nbsp;döngüsü__
*   __Kanalın azalan&nbsp;değeri__
*   __Reklamların organik listelemelere daha çok benzemesi__
*   __Örnek veri eksikliği__
*   __Ölçülemeyen şeyler__
*   __Yüksek maliyetler__

Veri biliminin SEO ile buluştuğu noktayı keşfettikçe, bu birleşmenin zorluklardan yoksun olmadığı açığa çıkar. Ancak, bu zorluklar, arama motoru dinamiklerinin labirentinden çıkarılabilecek aydınlatıcı görüşlerin potansiyelini gölgelemektedir. Bu karmaşık ilişki, dijital peyzajları yeniden şekillendirmek ve işletmeleri çevrimiçi ekosistemlerdeki her zaman ulaşılamayan üstünlüğe doğru yönlendirmek için güce&nbsp;sahiptir

>  ___Numune Veri Eksikliği___

Veri noktalarının eksikliği, veri odaklı SEO analizini daha zorlu hale getiren bir faktördür. Ne kadar sık bir SEO teknik denetim yapmıştır ve bunu SEO gerçeğinin bir yansıması olarak almıştır? Bu web sitesinin belirli bir denetim sırasında kötü bir anı yaşamadığını nereden bilebiliriz?  
Neyse ki, sektör lideri sıralama ölçüm araçları günlük bazda sıralamaları kaydediyor. Peki neden SEO ekipleri daha düzenli bir temelde denetim yapmıyor?  
Birçok SEO ekibi, birden fazla ölçüm yapmak için altyapıya sahip değil çünkü çoğunluğun buna sahip olma imkanı yok,&nbsp;çünkü

*   __Veri bilimi için birden fazla ölçümün değerini anlamıyorlar__
*   __Kaynakları veya altyapıyı bulundurmuyorlar__
*   __Web sitesi değişikliklerini bilmek için başka bir denetim yapmadan önce beklemeyi tercih ediyorlar (ContentKing gibi araçlar bu süreci otomatik hale getirse&nbsp;de)__

SEO gerçeğinin gerçek bir temsilini sağlayan bir veri kümesine sahip olmak için birden fazla denetim ölçümüne ihtiyaç vardır. Bu, günlük ortalama ve standart sapmalar gibi istatistiklere olanak tanır ve&nbsp;bunlar

*   __Sunucu durum&nbsp;kodları__
*   __Yinelemeli içerik__
*   __Eksik başlıklar__

Bu tür verilerle, veri bilimcileri anlamlı SEO bilimsel çalışmaları yapabilir ve bunları sıralamalar ve kullanıcı deneyimi sonuçlarıyla takip edebilirler.

>  ___Ölçülemez Şeyler___

Veriyi toplama konusunda en iyi niyetle bile, ölçmeye değer olan her şey ölçülemeyebilir. Bu muhtemelen sadece SEO değil, tüm pazarlama kanalları için geçerli bir durumdur; ancak bu, veri bilimcilerinin SEO’ya geçmemesinin en büyük nedeni değildir. Eğer bir şey söylemek gerekirse, SEO’da birçok şey ölçülebilir ve SEO veri bakımından zengindir.

Ölçmek istediğimiz bazı şeyler şunlar olabilir:

*   __Arama sorgusu:__ Google, bir süredir organik trafiğin arama sorgusu detayını gizliyor ve Google Analytics’teki anahtar kelime detayı “Sağlanmadı” olarak gösteriliyor. Elbette, bu bir URL’ye çok sayıda anahtar kelime ilişkisi olduğundan, ayrıntıların kırılımını elde etmek, dönüşüm modelleme sonuçları için (örneğin potansiyel müşteriler, siparişler ve gelirler) önemli olacaktır.
*   __Arama hacmi:__ Google Ads, arama sorgusu başına arama hacmini tam olarak açıklamaz. Uzun kuyruklu ifadeler için Ads tarafından sağlanan arama hacmi verisi, daha geniş eşleşmelere yeniden tahsis edilir çünkü Google için bu terimler üzerindeki kullanıcıların teklif vermesini teşvik etmek karlıdır, çünkü açık artırmada daha fazla teklif veren bulunur. Google Search Console (GSC) iyi bir alternatiftir, ancak ilk taraf verisidir ve hipotez anahtar kelimeniz için sitenizin varlığına son derece bağımlıdır.
*   __Segment:__ Bu, sadece anahtar kelimenin değil, kimin arama yaptığını bize söyler ve elbette bir milyonerin “erkek kot pantolon” aramasıyla daha mütevazı bir kullanıcının beklediği sonuçlar büyük ölçüde farklı olacaktır. Sonuçta, Google kişiselleştirilmiş sonuçlar sunuyor. Segmenti bilmemek, herhangi bir SERP modeline veya başka bir modele gürültü&nbsp;ekler.

>  ___Yüksek Miktarlar___

Botify gibi geniş ölçekli bir işletme tarama teknolojisiyi günlük olarak çalıştırmayı düşünün. Birçok marka maliyet nedeniyle bu taramayı ayda bir kez çalıştırır; ve sadece kendi siteniz için değil. Tam bir veri kümesine ulaşmak için rakiplerinizde de çalıştırmanız gerekecektir ve bu sadece bir tür SEO&nbsp;verisi.

Maliyet, reklam ajansı veri bilimcisi için o kadar önemli olmayabilir, ancak ajans veriye erişim sağlayıp sağlamayacağına etki edecektir çünkü ajans, bütçenin değmeyeceğine karar verebilir.

>  ___Neden SEO İçin Veri Bilimine Yönelmelisiniz?___

SEO kampanya ve operasyonlarınızı veri odaklı hale getirmek için veri bilimine yönelmeniz için birçok neden bulunmaktadır.

*   ___SEO Veri Açısından Zengin Bir Alandır&nbsp;: ___Her şeyi ölçmek için elimizde Google’ın Arama Motoru Sonuç Sayfalarında (SERP’ler) listelenen web sitelerine kullanıcı tepki verisi de dahil olmak üzere her şeyin verisi bulunmamaktadır; bu, nihai sonuç verileri olurdu. Sahip olduğumuz şey, birincil kaynak (sizin/sizin şirketinizin verileri gibi Google/Adobe Analytics) ve üçüncül kaynak (sıralama kontrol araçları, bulut denetimi yazılımları gibi) dışa aktarım verileridir.  
    Ayrıca, bu verileri anlamak için ücretsiz olan açık kaynak veri bilimi araçlarımız bulunmaktadır. Ayrıca, SEO verisinin sürekli artan selini anlamak için bu araçları nasıl kullanacağınızı öğretmeye istekli birçok ücretsiz ve son derece güvenilir çevrimiçi kaynak bulunmaktadır.
*   ___SEO Otomatize Edilebilir_: __En azından belirli yönlerde. Kariyerinizin tamamen robotlar tarafından ele geçirileceğini söylemiyoruz. Yine de, SEO uzmanı olarak yaptığınız işin bazı yönlerinin bilgisayarlar tarafından yerine getirilebileceği bir durum olduğuna inanıyoruz. Sonuçta, bilgisayarlar tekrarlayan görevleri yapma konusunda son derece iyidir, yorulmazlar veya sıkılmazlar, üç boyutun ötesini “görebilirler” ve sadece elektrikle çalışırlar.  
    Bilgisayarın kolayca yapabileceği tekrarlayan işleri yapmak, değer katmayan, duygusal olarak çekici olmayan ve zihinsel sağlığınız için iyi olmayan bir şeydir. Önemli olan, insan olarak en iyi olduğumuz zaman, bir müşterinin SEO çalışmaları hakkında düşünerek ve bilgiyi sentezleyerek çalışmamızdır; işte en iyi işimizi yaptığımız zaman&nbsp;budur.
*   ___Veri Bilimini Kullanmak Karlıdır_: __Bu verileri anlamak için ücretsiz olan açık kaynak veri bilimi araçlarına da sahibiz (R, Python). Ayrıca, SEO verisinin sürekli artan selini anlamak için bu araçları nasıl kullanacağınızı öğretmeye istekli birçok ücretsiz ve oldukça güvenilir çevrimiçi kaynak da bulunmaktadır.  
    Ayrıca, çok fazla veri varsa, Amazon Web Services (AWS) ve Google Cloud Platform (GCP) gibi bulut bilişim hizmetleri de saatlik kiralama ile kullanılabilir durumdadır.

>  ___Anahtar Kelime Araştırması___

Her arama motoruna giren kullanıcının arkasında bir kelime veya kelime dizisi bulunur. Örneğin, bir kullanıcı “otel” veya belki de “İstanbul’da otel” arayışında olabilir. Arama motoru optimizasyonunda (__SEO__), anahtar kelimeler kaçınılmaz olarak hedeflenen unsurlardır. Bu, belirli sorguların talebini anlamada yardımcı olan ve ürünler, hizmetler, organizasyonlar ve sonuç olarak cevaplar için kullanıcıların farklı arama yollarını daha etkili bir şekilde anlamada yardımcı olan faydalı bir&nbsp;yoldur.

Anahtar kelimelerle başlayan __SEO__’nun yanı sıra, bir __SEO__ kampanyası, anahtar kelimenin katkısının değerlendirilmesiyle sonlanma eğilimindedir. Bu bilgi Google tarafından bizden gizlense de, birçok __SEO __aracı, bir web sitesine ulaşmak için kullanıcıların kullandığı anahtarı çıkarmaya yönelik çabalar göstermiştir.

Şimdi, web siteniz için değerli anahtar kelimeleri bulmak için veri odaklı yöntemleri sunacağız (kullanıcı talebinin daha zengin bir anlayışına sahip olmanıza olanak tanımak&nbsp;için).

Ayrıca, anahtar kelime sıralama izleme maliyeti gerektirir (genellikle izlenen anahtar kelime başına ücretlendirilir veya toplamda izlenecek anahtar kelime sayısına sınırlama getirilir), bu nedenle hangi anahtar kelimelerin izleme maliyetine değer olduğunu bilmek mantıklıdır.

>  ___Anahtar Kelimeler İçin Veri Kaynakları___

Anahtar kelime araştırması konusunda kullanılabilecek birkaç veri kaynağı bulunmaktadır. Bunları aşağıdaki gibi sıralayabiliriz:

*   __Google Search&nbsp;Console__
*   __Rakip Analitikleri__
*   __SERP Sonuçları (Arama Motoru Sonuç Sayfaları)__
*   __Google Trends__
*   Google Ads
*   Google Önerileri

Kalın harflerle vurgulananları ele alacağız, çünkü bu veri kaynakları hem daha bilgilendirici hem de veri bilimi yöntemleri açısından ölçeklenebilir. Google Ads verileri yalnızca gerçek kullanım verilerine dayanıyorsa bir o kadar kullanışlı olurdu.

Ayrıca, hem 1. sayfa sıralaması (1 ila 10 arasındaki pozisyonlar arasında) elde ederseniz ne kadar izlenim alabileceğinizi hem de bu etkinin altı aylık bir süre boyunca nasıl olacağını tahmin etmenizi göstereceğiz.

Müşterilerin nasıl arama yaptığına dair ayrıntılı bir anlayışla, dizinlediğiniz yerin bu talebe göre nasıl bir konumda olduğunu karşılaştırarak (bu fırsata ne kadar yaklaşabileceğinizi anlamak için) daha güçlü bir konumda olursunuz ve aynı zamanda web sitenizi ve SEO faaliyetinizi bu talebi hedeflemek üzere müşteri odaklı hale getirirsiniz. Haydi başlayalım.

>  __Google Search Console&nbsp;(GSC)__

Google Search Console (GSC), zengin pazar istihbaratına sahip olan (ücretsiz) bir birinci taraf veri kaynağıdır. Google’ın API’sini tarih ve anahtar kelime düzeyinde sorgulama girişimlerinde veriyi çözümlemeyi zorlaştırmak için elinden geleni yapmasına ve hatta veriyi belirsizleştirmesine şaşmamalıdır.

Anahtar kelime araştırması söz konusu olduğunda GSC verileri uzmanların ilk başvurduğu öncelikli kaynaktır çünkü sayılar tutarlıdır ve üçüncü taraf verilerinin aksine sıralamaya haritalanan genel tıklama oranına dayanmayan veriler alırsınız.

Genel strateji, sıralama pozisyonlarına göre ortalamanın üzerinde belirgin bir şekilde görünen izlenimlere sahip arama sorgularını aramaktır. Neden izlenimler? Çünkü izlenimler daha bol miktarda bulunur ve fırsatı temsil eder, oysa tıklamalar genellikle sonradan gelir, yani fırsatın sonucudur. Ne anlam taşır? Örneğin, izlenim seviyeleri ortalamanın üzerinde iki standart sapmanın (sigma) üzerinde olan herhangi bir arama sorgusu olabilir.

>  ___Veriyi İçe Aktarma, Temizleme ve Düzenleme___

Öncelikle gerkeli kütüphaneleri kodumuza dahil&nbsp;edelim.

<pre>import pandas as pd<br/>import numpy as np<br/>import glob<br/>import os</pre>

Veri, bir dizi filtre temelinde en üst 1000 satırın Google Search Console (GSC) dışa aktarmalarından oluşuyor.

Şu an için, yerel bir klasörde depolanan birden çok GSC dışa aktarma dosyasını okuyoruz. Dosyaları okumak için yolunuzu ayarlayın:

<pre>data_dir = os.path.join('data', 'csvs')<br/>gsc_csvs = glob.glob(data_dir + "/*.csv")</pre>

Okuduğumuz veriyi kaydetmek için boş bir liste oluşturalım:

<pre>gsc_li = [] // or list()</pre>

Şimdi ise verilerimizi bir döngüye alarak yeni listemize ekleyelim:

<pre>for cf in gsc_csvs:<br/>    df = pd.read_csv(cf, index_col=None, header=0)<br/>    df['modifier'] = os.path.basename(cf)<br/>    df.modifier = df.modifier.str.replace('_queries.csv', '')<br/>    gsc_li.append(df)</pre>

Şimdi ise bu verileri tek bir çerçevede birleştirelim

<pre>gsc_raw_df = pd.DataFrame()<br/>gsc_raw_df = pd.concat(gsc_li, axis=0, ignore_index=True)</pre>

Şimdi ise sütunlarımızı daha kullanıcı dostu hale getirelim:

<pre>gsc_raw_df.columns = gsc_raw_df.columns.str.strip().str.lower().str.<br/>replace(' ', '_').str.replace('(', '').str.replace(')', '')<br/>gsc_raw_df.head()</pre>

Bu işlemden sonra sütunlar alttaki şekilde gözükecek:

<figure><img alt="" src="https://cdn-images-1.medium.com/max/912/1*XPg4_rQjM53EHkhr0DzOHw.png"/></figure>

Şimdi birde yüzdelik verilerimizi daha okunaklı hale getirebilmek için bir işlem&nbsp;yapalım:

<pre>gsc_clean_ctr_df['ctr'] = gsc_clean_ctr_df['ctr'].str.replace('%', '')<br/>gsc_clean_ctr_df['ctr'] = pd.to_numeric(gsc_clean_ctr_df['ctr'])</pre>

Ve bizim işimize yarar olan anahtar kelimeleri tespit etmek için 10 tıklanmanın altındaki anahtar kelimeleri silelim.

<pre>gsc_clean_ctr_df['impressions'] = gsc_clean_ctr_df.impressions.str.<br/>replace('&lt;', '')<br/>pd.to_numeric(gsc_import_df.impressions)</pre>

Son olarak ise top\_queries’e göre sıralayalım.

<pre>gsc_dedupe_df = gsc_clean_ctr_df.drop_duplicates(subset='top_queries',<br/>keep="first")</pre>

Şimdi ise sıra farklı bir bölümleme türünde.

>  ___Sorgu Türüne Göre&nbsp;Bölümle___

Bir sonraki adım, sorguları türe göre bölümlemektir. Bunun nedeni, genel web sitesi yerine bir segment içindeki izlenim hacimlerini karşılaştırmak istememizdir. Bu, sayıları segment içindeki fırsatları vurgulama açısından daha anlamlı hale getirir. Aksi takdirde, izlenimleri web sitesi ortalamasına göre karşılaştırsak, değerli arama sorgusu fırsatlarını kaçırabiliriz. Python’da kullandığımız yaklaşım, sorgu sütununda bulunan değiştirici dizeler temelinde kategorilendirmektir:

<pre>retail_vex = ['cdkeys', 'argos', 'smyth', 'amazon', 'cyberpunk', 'GAME']<br/>platform_vex = ['ps5', 'xbox', 'playstation', 'switch', 'ps4', 'nintendo']<br/>title_vex = ['blackops', 'pokemon', 'minecraft', 'mario',<br/>'outriders','fifa', 'animalcrossing', 'resident', 'spiderman',<br/>'newhorizons', 'callofduty']<br/>network_vex = ['ee', 'o2', 'vodafone','carphone']<br/>gsc_segment_strdetect = gsc_dedupe_df[['query', 'clicks', 'impressions',<br/>'ctr', 'position']]</pre>

Koşullarımızın bir listesini oluşturalım:

<pre>query_conds = [<br/>    gsc_segment_strdetect['query'].str.contains('|'.join(retail_vex)),<br/>    gsc_segment_strdetect['query'].str.contains('|'.join(platform_vex)),<br/>    gsc_segment_strdetect['query'].str.contains('|'.join(title_vex)),<br/>    gsc_segment_strdetect['query'].str.contains('|'.join(network_vex))<br/>]</pre>

Her bir koşul için atamak istediğimiz değerlerin bir listesini oluşturalım:

<pre>segment_values = ['Retailer', 'Console', 'Title', 'Network'] #, 'Title',<br/>'Accessories', 'Network', 'Top1000', 'Broadband']</pre>

Yeni bir sütun oluşturun ve np.select’i kullanarak argüman olarak listelerimizi kullanarak değerleri atayalım:

<pre>gsc_segment_strdetect['segment'] = np.select(query_conds, segment_values)<br/>gsc_segment_strdetect</pre>

Çıktımız aşağıdaki gibi olacaktır:

<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*GK9CR4KaX3Qw3RAIP1QhPg.png"/></figure>

>  Şimdi ise isterseniz posizyon verilerimizi daha kullanışlı olması açısından tam sayılara yuvarlayalım.

<pre>gsc_segment_strdetect['rank_bracket'] = gsc_segment_strdetect.position.<br/>round(0)<br/>gsc_segment_strdetect</pre>

Kodumuzun son çıktısı aşağıdaki gibi olacaktır:

<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*wO14_K1OGgdbXyEAQw8pEA.png"/></figure>

>  Şimdi ise bir adım ileriye giderek Segment Ortalamasını ve Değişkenliğini hesaplayalım

Şimdi veriyi bölümlere ayırdık, ortalama izlenimleri ve sıralama pozisyonu için alt ve üst yüzde dilimlerini hesaplıyoruz. Amaç, izlenimlerin, sıralama pozisyonundan iki standart sapma veya daha fazla yukarıda olduğu sorguları belirlemektir.

Bu, sorgunun muhtemelen SEO için büyük bir fırsat olduğu ve takip etmeye değer olduğu anlamına gelir.  
Sadece yüksek izlenimli anahtar kelimeleri seçmek yerine bunu bu şekilde yapıyoruz, çünkü birçok anahtar kelime sorgusu zaten ilk 20'de olmaları nedeniyle yüksek izlenimlere sahiptir. Bu, takip edilmesi gereken sorgu sayısını oldukça büyük ve maliyetli hale getirebilir.

<pre>queries_rank_imps = gsc_segment_strdetect[['rank_bracket', 'impressions']]<br/>group_by_rank_bracket = queries_rank_imps.groupby(['rank_bracket'], as_<br/>index=False)<br/>def imp_aggregator(col):<br/>    d = {}<br/>    d['avg_imps'] = col['impressions'].mean()<br/>    d['imps_median'] = col['impressions'].quantile(0.5)<br/>    d['imps_lq'] = col['impressions'].quantile(0.25)<br/>    d['imps_uq'] = col['impressions'].quantile(0.95)<br/>    d['n_count'] = col['impressions'].count()<br/>    return pd.Series(d, index=['avg_imps', 'imps_median', 'imps_lq', 'imps_uq', 'n_count'])<br/>overall_rankimps_agg = group_by_rank_bracket.apply(imp_aggregator)<br/>overall_rankimps_agg</pre>

Ve son aşamada kodumuz çalıştığında bu şekilde bir tablo karşımıza çıkacaktır.

<figure><img alt="" src="https://cdn-images-1.medium.com/max/912/1*-g104qAlX3723uABG7vvTg.png"/></figure>

Bu durumda, 25. ve 95. yüzde dilimleriyle ilerledik. Alt yüzde dilimi sayısı pek önemli değil, çünkü ortalama değerleri 95. yüzde dilimin ötesinde olan sorguları bulmaktan daha fazla ilgileniyoruz. Bunu başarabilirsek, lezzetli bir anahtar kelime bulmuş oluruz. Hızlı bir not olarak, veri biliminde yüzde dilimi “çeyreklik” olarak&nbsp;bilinir.

Her bir bölüm için ayrı bir tablo oluşturabilir miyiz? Örneğin, bölüme göre sıralama pozisyonuna göre izlenim istatistiklerini gösterin. Evet, elbette yapabilirsiniz ve teorik olarak, sorguların bölüm ortalamasına göre nasıl performans gösterdiğine dair daha bağlamsal bir analiz sağlardı. Bunun yapılıp yapılmayacağına karar verme faktörü, her sıralama kategorisi için kaç veri noktanızın (yani sıralı sorgular) olduğuna ve bunu yapmaya değip değmeyeceğine (yani istatistiksel olarak güvenilir) bağlıdır. Bu tür bir analiz için her birinde en az 30 veri noktasına ihtiyaç duyarsınız.

>  Ortalama İzlenim Seviyelerini Karşılaştıralım

Şimdi önceki veri setinden gelen tabloyu birleştirelim (vlookup veya index match gibi düşünün) ve ardından bölümlendirilmiş verilere birleştirelim. Sonuç olarak, sorgu verilerini beklenen ortalama ve üst çeyreklikle karşılaştıran bir veri çerçevemiz olacak.

accessory\_queries tablosunu accessories\_rankimps\_agg ile rank\_bracket’e göre birleştirin (vlookup veya index match gibi düşünün).

<pre>query_quantile_stats = gsc_segment_strdetect.merge(overall_rankimps_agg, on <br/>=['rank_bracket'], how='left')<br/>query_quantile_stats</pre>

Tablomuzun son hali bu şekilde olacaktır.

<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*zEapM3VVydESmhhnIt7NGw.png"/></figure>

>  Veriyi Keşfedelim

Şimdi merak ediyor olabilirsiniz, kaç anahtar kelime ağırlığının üstünde ve altında (yani sıralama pozisyonuna göre çeyreklik sınırlarının üstünde ve altında) ve bu anahtar kelimeler neler?  
Yüksek izlenim hacmine sahip anahtar kelimelerin sayısını&nbsp;alın:

<pre>query_stats_uq = query_quantile_stats.loc[query_quantile_stats.impressions<br/>&gt; query_quantile_stats.imps_uq]<br/>query_stats_uq['query'].count()</pre>

Ekrana yazılacak olan sonuç __8390__ olacaktır.

İzlenim dağılımının sıralama pozisyonları aralığında görsel olarak nasıl göründüğüne bakalım:

<pre>import seaborn as sns<br/>import matplotlib.pyplot as plt<br/>from pylab import savefig<br/><br/>sns.set(rc={'figure.figsize':(15, 6)})<br/><br/>imprank_plt = sns.relplot(x = "rank_bracket", y = "impressions",<br/>                hue = "quantiled", style = "quantiled",<br/>                kind = "line", data = overall_rankimps_agg_long)<br/>imprank_plt.savefig("images/imprank_plt.png")</pre>

İlginç olan, üst çeyreklik izlenim anahtar kelimelerinin hepsinin en üst 10'da olmadığıdır. Bu, sitenin ya yüksek hacimli anahtar kelimeleri hedeflediğini ancak yüksek bir sıralama pozisyonu elde etme konusunda başarılı olamadığını ya da bu yüksek hacimli ifadeleri hedeflemediğini gösterir.

<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*FQA0LyqdX8ERtqX3SfCpXw.png"/></figure>

Bu bölümü detaylı olarak inceleyelim.  
İzlenimleri sıralama aralığına göre bölümle&nbsp;çizelim:

<pre>imprank_seg = sns.relplot(x="rank_bracket", y="impressions",<br/>                hue="quantiled", col="segment",<br/>                kind="line", data = overall_rankimps_agg_long, facet_<br/>                kws=dict(sharex=False))<br/>imprank_seg.savefig("images/imprank_seg.png")</pre>

Yüksek izlenimli anahtar kelimelerin çoğu Aksesuarlar, Konsol ve tabii ki En İyi 1000 kategorilerinde bulunuyor:

<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*2oxdA2nVHNZoSk_Hk3PS_w.png"/></figure>

>  Veriyi CSV Olarak Dışa Aktaralım

<pre>query_stats_uq_p2b.to_csv('exports/verbilimi_seo.csv')</pre>

<img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=540f812a5790" width="1"/>